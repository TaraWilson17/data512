{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection\n",
    "\n",
    "This notebook walks through the process of collecting the data needed for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "### Setup\n",
    "\n",
    "This Jupyter Notebook is created using [Python version 3.7](https://www.python.org/downloads/release/python-370/).\n",
    "\n",
    "First, I will import the necessary libraries to run the code. The following libraries are used:  \n",
    "* [json 2.0.9](https://docs.python.org/3/library/json.html)  \n",
    "* [requests 2.22.0](https://pypi.org/project/requests/) \n",
    "* [time](https://docs.python.org/3/library/time.html)  \n",
    "* [pandas 0.25.3](https://pandas.pydata.org/)\n",
    "* [datetime](https://docs.python.org/3/library/datetime.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "\n",
    "### Function definitions\n",
    "\n",
    "This section defines functions to be used later in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `make_and_save_api_call` function makes an API call to the given url endpoint with the given parameters. The json respoonse from the call is saved with a `.json` extension to the given location and with the given file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_and_save_api_call(endpoint, params, filename):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - endoint: URL for the API call to go to\n",
    "        - params: Parameters to be passed with the API call\n",
    "        - filename: File path and name to save json response to\n",
    "    outputs:\n",
    "        - No returns. Response is saved to .json file.\n",
    "    \"\"\"\n",
    "    call = requests.get(endpoint.format(**params))\n",
    "    response = call.json()\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(response, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_noaa_data` function makes an API call to the given url endpoint to gether the given datatype with the given header information. The json response from the API call is returned from the function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noaa_data(endpoint, data_type, header):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "        - endpoint: URL for the API call to go to\n",
    "        - data_type: Type of NOAA data to query for\n",
    "        - header: Header information for call (Should be a dictionary with API access token)\n",
    "    outputs:\n",
    "        - Returns json response from call\n",
    "    \"\"\"\n",
    "    r = requests.get(endpoint, data_type, headers=header)\n",
    "    return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "### College football data API Calls\n",
    "\n",
    "Details about the college football dataset are found [here](https://github.com/BlueSCar/cfb-database) and is aggregated by GitHub user [Bill Radjewski](https://github.com/BlueSCar). This data is available under an [MIT License](https://github.com/BlueSCar/cfb-database/blob/master/LICENSE). \n",
    "\n",
    "The schema for the data is as follows:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](https://github.com/BlueSCar/cfb-database/blob/master/SchemaDiagram.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I accessed this data using the [Swagger API endpoints](https://api.collegefootballdata.com/api/docs/?url=/api-docs.json).\n",
    "\n",
    "-----------------\n",
    "\n",
    "### Game data\n",
    "\n",
    "The first endpoint I called is the games data point accessed via `https://api.collegefootballdata.com/games`.\n",
    "\n",
    "The games data call takes the following parameters:  \n",
    "\n",
    "| name     | required | description |\n",
    "| ---------|----------|-------------|\n",
    "| year     | yes      | year/season filter for games |\n",
    "| week     | no       | week filter |\n",
    "| seasonType| no | season type filter|\n",
    "| team     | no | team name|\n",
    "| home     | no | home team filter |\n",
    "| away     | no | away team filter |\n",
    "| conference| no | conference abbreviation filter|\n",
    "| id| no | id filter for querying a single game|\n",
    "\n",
    "I wanted all game data so I only passed the year parameter. This returns all reqular season games from that year in the following format:  \n",
    "\n",
    "```json\n",
    "  {\n",
    "    \"id\": 401013357,\n",
    "    \"season\": 2018,\n",
    "    \"week\": 1,\n",
    "    \"season_type\": \"regular\",\n",
    "    \"start_date\": \"2018-08-25T21:30:00.000Z\",\n",
    "    \"neutral_site\": false,\n",
    "    \"conference_game\": false,\n",
    "    \"attendance\": 8684,\n",
    "    \"venue_id\": 3985,\n",
    "    \"venue\": \"Warren McGuirk Alumni Stadium\",\n",
    "    \"home_team\": \"UMass\",\n",
    "    \"home_conference\": \"FBS Independents\",\n",
    "    \"home_points\": 63,\n",
    "    \"home_line_scores\": [\n",
    "      21,\n",
    "      14,\n",
    "      21,\n",
    "      7\n",
    "    ],\n",
    "    \"home_post_win_prob\": \"0.9984542422162311\",\n",
    "    \"away_team\": \"Duquesne\",\n",
    "    \"away_conference\": null,\n",
    "    \"away_points\": 15,\n",
    "    \"away_line_scores\": [\n",
    "      3,\n",
    "      6,\n",
    "      0,\n",
    "      6\n",
    "    ],\n",
    "    \"away_post_win_prob\": \"0.001545757783768864\"\n",
    "  }\n",
    "```\n",
    "\n",
    "I wil then make the API calls to the following endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_endpoint = \"https://api.collegefootballdata.com/games?year={year}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no attendance data recorded for seasons before 2011. Therefore, I will be looking at the 2012-2018 seasons so I will make 7 calls, 1 each to get data for all of these years and save them to their corresponding `.json` extension files using the `make_and_save_api_call` defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_and_save_api_call(games_endpoint, {\"year\": \"2012\"}, \"raw_data/games_endpoint_2012.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2013\"}, \"raw_data/games_endpoint_2013.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2014\"}, \"raw_data/games_endpoint_2014.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2015\"}, \"raw_data/games_endpoint_2015.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2016\"}, \"raw_data/games_endpoint_2016.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2017\"}, \"raw_data/games_endpoint_2017.json\")\n",
    "make_and_save_api_call(games_endpoint, {\"year\": \"2018\"}, \"raw_data/games_endpoint_2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will then read all of these in from their corresponding json files into DataFrames using [panda's `read_json` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "games_2012 = pd.read_json(\"raw_data/games_endpoint_2012.json\")\n",
    "games_2013 = pd.read_json(\"raw_data/games_endpoint_2013.json\")\n",
    "games_2014 = pd.read_json(\"raw_data/games_endpoint_2014.json\")\n",
    "games_2015 = pd.read_json(\"raw_data/games_endpoint_2015.json\")\n",
    "games_2016 = pd.read_json(\"raw_data/games_endpoint_2016.json\")\n",
    "games_2017 = pd.read_json(\"raw_data/games_endpoint_2017.json\")\n",
    "games_2018 = pd.read_json(\"raw_data/games_endpoint_2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will then join all 7 seasons into one DataFrame named `all_games` using [panda's `concat` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) and we can preview the data available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attendance</th>\n",
       "      <th>away_conference</th>\n",
       "      <th>away_line_scores</th>\n",
       "      <th>away_points</th>\n",
       "      <th>away_post_win_prob</th>\n",
       "      <th>away_team</th>\n",
       "      <th>conference_game</th>\n",
       "      <th>home_conference</th>\n",
       "      <th>home_line_scores</th>\n",
       "      <th>home_points</th>\n",
       "      <th>home_post_win_prob</th>\n",
       "      <th>home_team</th>\n",
       "      <th>id</th>\n",
       "      <th>neutral_site</th>\n",
       "      <th>season</th>\n",
       "      <th>season_type</th>\n",
       "      <th>start_date</th>\n",
       "      <th>venue</th>\n",
       "      <th>venue_id</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12616.0</td>\n",
       "      <td>Conference USA</td>\n",
       "      <td>[7, 28, 14, 7]</td>\n",
       "      <td>56</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>UCF</td>\n",
       "      <td>False</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 0, 7, 7]</td>\n",
       "      <td>14</td>\n",
       "      <td>0.203786</td>\n",
       "      <td>Akron</td>\n",
       "      <td>322432006</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular</td>\n",
       "      <td>2012-08-30T19:00:00.000Z</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>3768.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38393.0</td>\n",
       "      <td>SEC</td>\n",
       "      <td>[7, 3, 0, 7]</td>\n",
       "      <td>17</td>\n",
       "      <td>0.518995</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>True</td>\n",
       "      <td>SEC</td>\n",
       "      <td>[0, 10, 3, 0]</td>\n",
       "      <td>13</td>\n",
       "      <td>0.481005</td>\n",
       "      <td>Vanderbilt</td>\n",
       "      <td>322430238</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular</td>\n",
       "      <td>2012-08-30T19:00:00.000Z</td>\n",
       "      <td>Vanderbilt Stadium</td>\n",
       "      <td>3973.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12725.0</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 13, 0, 13]</td>\n",
       "      <td>26</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>Eastern Michigan</td>\n",
       "      <td>True</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[10, 3, 21, 3]</td>\n",
       "      <td>37</td>\n",
       "      <td>0.979920</td>\n",
       "      <td>Ball State</td>\n",
       "      <td>322432050</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular</td>\n",
       "      <td>2012-08-30T19:00:00.000Z</td>\n",
       "      <td>Scheumann Stadium</td>\n",
       "      <td>3919.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15250.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[10, 14, 3, 0]</td>\n",
       "      <td>27</td>\n",
       "      <td>0.369692</td>\n",
       "      <td>Southeast Missouri State</td>\n",
       "      <td>False</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[10, 14, 7, 7]</td>\n",
       "      <td>38</td>\n",
       "      <td>0.630308</td>\n",
       "      <td>Central Michigan</td>\n",
       "      <td>322432117</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular</td>\n",
       "      <td>2012-08-30T19:00:00.000Z</td>\n",
       "      <td>Kelly/Shorts Stadium</td>\n",
       "      <td>3786.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15121.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[0, 7, 0, 14]</td>\n",
       "      <td>21</td>\n",
       "      <td>0.644572</td>\n",
       "      <td>Towson</td>\n",
       "      <td>False</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[17, 10, 7, 7]</td>\n",
       "      <td>41</td>\n",
       "      <td>0.355428</td>\n",
       "      <td>Kent State</td>\n",
       "      <td>322432309</td>\n",
       "      <td>False</td>\n",
       "      <td>2012</td>\n",
       "      <td>regular</td>\n",
       "      <td>2012-08-30T19:00:00.000Z</td>\n",
       "      <td>Dix Stadium</td>\n",
       "      <td>3696.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   attendance away_conference away_line_scores  away_points  \\\n",
       "0     12616.0  Conference USA   [7, 28, 14, 7]           56   \n",
       "1     38393.0             SEC     [7, 3, 0, 7]           17   \n",
       "2     12725.0    Mid-American   [0, 13, 0, 13]           26   \n",
       "3     15250.0            None   [10, 14, 3, 0]           27   \n",
       "4     15121.0            None    [0, 7, 0, 14]           21   \n",
       "\n",
       "   away_post_win_prob                 away_team  conference_game  \\\n",
       "0            0.796214                       UCF            False   \n",
       "1            0.518995            South Carolina             True   \n",
       "2            0.020080          Eastern Michigan             True   \n",
       "3            0.369692  Southeast Missouri State            False   \n",
       "4            0.644572                    Towson            False   \n",
       "\n",
       "  home_conference home_line_scores  home_points  home_post_win_prob  \\\n",
       "0    Mid-American     [0, 0, 7, 7]           14            0.203786   \n",
       "1             SEC    [0, 10, 3, 0]           13            0.481005   \n",
       "2    Mid-American   [10, 3, 21, 3]           37            0.979920   \n",
       "3    Mid-American   [10, 14, 7, 7]           38            0.630308   \n",
       "4    Mid-American   [17, 10, 7, 7]           41            0.355428   \n",
       "\n",
       "          home_team         id  neutral_site  season season_type  \\\n",
       "0             Akron  322432006         False    2012     regular   \n",
       "1        Vanderbilt  322430238         False    2012     regular   \n",
       "2        Ball State  322432050         False    2012     regular   \n",
       "3  Central Michigan  322432117         False    2012     regular   \n",
       "4        Kent State  322432309         False    2012     regular   \n",
       "\n",
       "                 start_date                              venue  venue_id  week  \n",
       "0  2012-08-30T19:00:00.000Z  Summa Field at InfoCision Stadium    3768.0     1  \n",
       "1  2012-08-30T19:00:00.000Z                 Vanderbilt Stadium    3973.0     1  \n",
       "2  2012-08-30T19:00:00.000Z                  Scheumann Stadium    3919.0     1  \n",
       "3  2012-08-30T19:00:00.000Z               Kelly/Shorts Stadium    3786.0     1  \n",
       "4  2012-08-30T19:00:00.000Z                        Dix Stadium    3696.0     1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_games = pd.concat([games_2012, games_2013, games_2014, games_2015, games_2016, games_2017, games_2018], ignore_index=True)\n",
    "all_games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I am doing an analysis on attendance, I want to only use data with correctly recorded attendance values (should be over 0). I will check how many rows of data have attendances = 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 911 rows of data with attendance = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"There are %d rows of data with attendance = 0\" % len(all_games[all_games[\"attendance\"] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is only a small fraction of the overall data, I will just exclude these rows from the dataset by filtering only for rows with attendance > 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_games = all_games[all_games[\"attendance\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venue data\n",
    "\n",
    "The other endpoint I called is the vanues data point accessed via `https://api.collegefootballdata.com/venues`.\n",
    "\n",
    "The venues endpoint does not take any parameters.\n",
    "\n",
    "The venues endpoint returns a list of all stadiums that hosted games included in the dataset. An example return is the following:\n",
    "\n",
    "```json\n",
    "  {\n",
    "    \"id\": 3601,\n",
    "    \"name\": \"Aggie Memorial Stadium\",\n",
    "    \"capacity\": 30343,\n",
    "    \"grass\": false,\n",
    "    \"city\": \"Las Cruces\",\n",
    "    \"state\": \"NM\",\n",
    "    \"zip\": \"88003\",\n",
    "    \"country_code\": \"US\",\n",
    "    \"location\": {\n",
    "      \"x\": 32.2796202,\n",
    "      \"y\": -106.7411148\n",
    "    },\n",
    "    \"elevation\": \"1208.201294\",\n",
    "    \"year_constructed\": 1978,\n",
    "    \"dome\": false\n",
    "  }\n",
    "```\n",
    "\n",
    "I will then make the API call to this endpoint to gather venue level information using the same `make_and_save_api_call` function from above. I will also save this to a `.json` file and then read it in as a DataFrame to print a preview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>capacity</th>\n",
       "      <th>city</th>\n",
       "      <th>country_code</th>\n",
       "      <th>dome</th>\n",
       "      <th>elevation</th>\n",
       "      <th>grass</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>year_constructed</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>Tuskegee</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4747</td>\n",
       "      <td>None</td>\n",
       "      <td>Abbott Memorial Alumni Stadium</td>\n",
       "      <td>AL</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28500</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.242132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3599</td>\n",
       "      <td>{'x': 30.5221461, 'y': -91.1896087}</td>\n",
       "      <td>Ace W. Mumford Stadium</td>\n",
       "      <td>LA</td>\n",
       "      <td>1928.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30343</td>\n",
       "      <td>Las Cruces</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1208.201294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3601</td>\n",
       "      <td>{'x': 32.2796202, 'y': -106.7411148}</td>\n",
       "      <td>Aggie Memorial Stadium</td>\n",
       "      <td>NM</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>88003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21500</td>\n",
       "      <td>Greensboro</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>235.122452</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3603</td>\n",
       "      <td>{'x': 36.0814337, 'y': -79.7700391}</td>\n",
       "      <td>Aggie Stadium</td>\n",
       "      <td>NC</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10849</td>\n",
       "      <td>Davis</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.382592</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3602</td>\n",
       "      <td>{'x': 38.5365266, 'y': -121.7627936}</td>\n",
       "      <td>Aggie Stadium</td>\n",
       "      <td>CA</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   capacity         city country_code  dome    elevation  grass    id  \\\n",
       "0     10000     Tuskegee           US   0.0   122.800000    NaN  4747   \n",
       "1     28500  Baton Rouge           US   0.0    20.242132    NaN  3599   \n",
       "2     30343   Las Cruces           US   0.0  1208.201294    0.0  3601   \n",
       "3     21500   Greensboro           US   0.0   235.122452    NaN  3603   \n",
       "4     10849        Davis           US   0.0    11.382592    NaN  3602   \n",
       "\n",
       "                               location                            name state  \\\n",
       "0                                  None  Abbott Memorial Alumni Stadium    AL   \n",
       "1   {'x': 30.5221461, 'y': -91.1896087}          Ace W. Mumford Stadium    LA   \n",
       "2  {'x': 32.2796202, 'y': -106.7411148}          Aggie Memorial Stadium    NM   \n",
       "3   {'x': 36.0814337, 'y': -79.7700391}                   Aggie Stadium    NC   \n",
       "4  {'x': 38.5365266, 'y': -121.7627936}                   Aggie Stadium    CA   \n",
       "\n",
       "   year_constructed      zip  \n",
       "0            1925.0      NaN  \n",
       "1            1928.0      NaN  \n",
       "2            1978.0  88003.0  \n",
       "3            1981.0      NaN  \n",
       "4            2007.0      NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues_endpoint = \"https://api.collegefootballdata.com/venues\"\n",
    "make_and_save_api_call(venues_endpoint, {}, \"raw_data/venues_endpoint.json\")\n",
    "venues = pd.read_json(\"raw_data/venues_endpoint.json\")\n",
    "venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "### Game and venue data combination\n",
    "\n",
    "All venues have a unique ID and `venue_id` is included in the games datset as well. Therefore, we can join the two datasets based on this field. \n",
    "\n",
    "First, we rename the `id` field in the venues dataset to `venue_id` so the name matches between the two DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues = venues.rename(columns={\"id\": \"venue_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use [panda's `merge` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) to combine the two datasets on the `venue_id` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data = pd.merge(all_games, venues, on=\"venue_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can preview the dataset now and see that the columns on the right are indeed venue level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attendance</th>\n",
       "      <th>away_conference</th>\n",
       "      <th>away_line_scores</th>\n",
       "      <th>away_points</th>\n",
       "      <th>away_post_win_prob</th>\n",
       "      <th>away_team</th>\n",
       "      <th>conference_game</th>\n",
       "      <th>home_conference</th>\n",
       "      <th>home_line_scores</th>\n",
       "      <th>home_points</th>\n",
       "      <th>...</th>\n",
       "      <th>city</th>\n",
       "      <th>country_code</th>\n",
       "      <th>dome</th>\n",
       "      <th>elevation</th>\n",
       "      <th>grass</th>\n",
       "      <th>location</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>year_constructed</th>\n",
       "      <th>zip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12616.0</td>\n",
       "      <td>Conference USA</td>\n",
       "      <td>[7, 28, 14, 7]</td>\n",
       "      <td>56</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>UCF</td>\n",
       "      <td>False</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 0, 7, 7]</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>Akron</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.287506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'x': 41.0725534, 'y': -81.5083408}</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>OH</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>44399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9933.0</td>\n",
       "      <td>None</td>\n",
       "      <td>[3, 3, 0, 0]</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>Morgan State</td>\n",
       "      <td>False</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[14, 7, 24, 21]</td>\n",
       "      <td>66</td>\n",
       "      <td>...</td>\n",
       "      <td>Akron</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.287506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'x': 41.0725534, 'y': -81.5083408}</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>OH</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>44399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8211.0</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[13, 22, 10, 11]</td>\n",
       "      <td>56</td>\n",
       "      <td>0.052170</td>\n",
       "      <td>Miami (OH)</td>\n",
       "      <td>True</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[14, 14, 7, 14]</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>Akron</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.287506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'x': 41.0725534, 'y': -81.5083408}</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>OH</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>44399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10102.0</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 0, 17, 7]</td>\n",
       "      <td>24</td>\n",
       "      <td>0.521928</td>\n",
       "      <td>Bowling Green</td>\n",
       "      <td>True</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 10, 0, 0]</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>Akron</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.287506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'x': 41.0725534, 'y': -81.5083408}</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>OH</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>44399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7074.0</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[10, 10, 10, 7]</td>\n",
       "      <td>37</td>\n",
       "      <td>0.699729</td>\n",
       "      <td>Northern Illinois</td>\n",
       "      <td>True</td>\n",
       "      <td>Mid-American</td>\n",
       "      <td>[0, 7, 0, 0]</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>Akron</td>\n",
       "      <td>US</td>\n",
       "      <td>0.0</td>\n",
       "      <td>321.287506</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'x': 41.0725534, 'y': -81.5083408}</td>\n",
       "      <td>Summa Field at InfoCision Stadium</td>\n",
       "      <td>OH</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>44399.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   attendance away_conference  away_line_scores  away_points  \\\n",
       "0     12616.0  Conference USA    [7, 28, 14, 7]           56   \n",
       "1      9933.0            None      [3, 3, 0, 0]            6   \n",
       "2      8211.0    Mid-American  [13, 22, 10, 11]           56   \n",
       "3     10102.0    Mid-American     [0, 0, 17, 7]           24   \n",
       "4      7074.0    Mid-American   [10, 10, 10, 7]           37   \n",
       "\n",
       "   away_post_win_prob          away_team  conference_game home_conference  \\\n",
       "0            0.796214                UCF            False    Mid-American   \n",
       "1            0.000028       Morgan State            False    Mid-American   \n",
       "2            0.052170         Miami (OH)             True    Mid-American   \n",
       "3            0.521928      Bowling Green             True    Mid-American   \n",
       "4            0.699729  Northern Illinois             True    Mid-American   \n",
       "\n",
       "  home_line_scores  home_points  ...   city country_code  dome   elevation  \\\n",
       "0     [0, 0, 7, 7]           14  ...  Akron           US   0.0  321.287506   \n",
       "1  [14, 7, 24, 21]           66  ...  Akron           US   0.0  321.287506   \n",
       "2  [14, 14, 7, 14]           49  ...  Akron           US   0.0  321.287506   \n",
       "3    [0, 10, 0, 0]           10  ...  Akron           US   0.0  321.287506   \n",
       "4     [0, 7, 0, 0]            7  ...  Akron           US   0.0  321.287506   \n",
       "\n",
       "   grass                             location  \\\n",
       "0    0.0  {'x': 41.0725534, 'y': -81.5083408}   \n",
       "1    0.0  {'x': 41.0725534, 'y': -81.5083408}   \n",
       "2    0.0  {'x': 41.0725534, 'y': -81.5083408}   \n",
       "3    0.0  {'x': 41.0725534, 'y': -81.5083408}   \n",
       "4    0.0  {'x': 41.0725534, 'y': -81.5083408}   \n",
       "\n",
       "                                name state  year_constructed      zip  \n",
       "0  Summa Field at InfoCision Stadium    OH            2009.0  44399.0  \n",
       "1  Summa Field at InfoCision Stadium    OH            2009.0  44399.0  \n",
       "2  Summa Field at InfoCision Stadium    OH            2009.0  44399.0  \n",
       "3  Summa Field at InfoCision Stadium    OH            2009.0  44399.0  \n",
       "4  Summa Field at InfoCision Stadium    OH            2009.0  44399.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attendance_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "### NOAA weather data API Calls\n",
    "\n",
    "In this section, I will make calls to the NOAA weather service to retrieve gameday rain and temperature data. Details on the API can be found [here](https://www.ncdc.noaa.gov/cdo-web/webservices/v2#gettingStarted).\n",
    "\n",
    "To complete the API call, you must request a Web Services Token by signing up with your email [here](https://www.ncdc.noaa.gov/cdo-web/token). This must be passed in a dictionary as a credential for the API call to return results.\n",
    "\n",
    "I wanted to collect temperature and rain data for all the games in the dataset. To do so, I make a call to the Daily Summary dataset (GHCND) which contains historical sumaries. More information can be found on this dataset [here](https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets/GHCND).\n",
    "\n",
    "The API call takes several parameters:\n",
    "\n",
    "| name    | description     | values |\n",
    "|---------|-----------------|--------|\n",
    "|locationid|location identifier information|ZIP:(zip code of venue)|\n",
    "|startdate|start date of data collection|(month-date-year of game)|\n",
    "|enddate|end date of data collection|(month-date-year of game)|\n",
    "\n",
    "I make API calls to this endpoint for all games included in the dataset, passing in the corresponding zip code, and game date as a string for both the start and end date as we only need information for that one day. Note: There is a threshold of API calls allowed in a period of time so I added a sleep statement for 180 seconds every 1000 calls. \n",
    "\n",
    "I first needed to convert the columns to the correct types to make the API call. I first conver the zip to int, following [this example](https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int) of how to deal with integer type in Python when there are NaN values and also convert the game date to the correctly formatted date type for the API call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data[\"zip\"] = attendance_data[\"zip\"].astype('Int64')\n",
    "attendance_data[\"start_date\"] = pd.to_datetime(attendance_data[\"start_date\"]).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The call is thenmade using the `get_noaa_data` function defined earlier and the JSON reponses are appended to a `weather_responses` list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='www.ncdc.noaa.gov', port=443): Max retries exceeded with url: /cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:85287&startdate=2015-09-18&enddate=2015-09-18&dataset (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000028BE69048D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    732\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    734\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    333\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             )\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x0000028BE69048D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='www.ncdc.noaa.gov', port=443): Max retries exceeded with url: /cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:85287&startdate=2015-09-18&enddate=2015-09-18&dataset (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000028BE69048D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5a6d5ab73300>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattendance_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mendpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:%s&startdate=%s&enddate=%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mweather_responses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_noaa_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m180\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-491a3bea92a1>\u001b[0m in \u001b[0;36mget_noaa_data\u001b[1;34m(endpoint, data_type, header)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m-\u001b[0m \u001b[0mReturns\u001b[0m \u001b[0mjson\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \"\"\"\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\interpret\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='www.ncdc.noaa.gov', port=443): Max retries exceeded with url: /cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:85287&startdate=2015-09-18&enddate=2015-09-18&dataset (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000028BE69048D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',))"
     ]
    }
   ],
   "source": [
    "weather_responses = []\n",
    "\n",
    "token = \"put_toke_here\"\n",
    "creds = dict(token=token)\n",
    "dtype = \"dataset\"\n",
    "\n",
    "for index, row in attendance_data.iterrows():\n",
    "    endpoint = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data?datasetid=GHCND&locationid=ZIP:%s&startdate=%s&enddate=%s\" % (row.zip, row.start_date, row.start_date)\n",
    "    weather_responses.append(get_noaa_data(endpoint, dtype, creds))\n",
    "    if index % 1000 == 0:\n",
    "        time.sleep(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw JSON values need to be parsed to pull out the precipitation and temperature data from the daily summary. To do so we need to grab the values for several fields:  \n",
    "* PRCP = precipitation for that day\n",
    "* TMAX = maximum temperature for that day\n",
    "* TMIN = minimum temperature for that day\n",
    "\n",
    "We can then pull out the results for these values by accessing the value for the JSON field with that identifier and aggregate them in lists for `rain`, `temp_max`, and `temp_min`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = []\n",
    "temp_max = []\n",
    "temp_min = []\n",
    "\n",
    "for response in weather_responses:\n",
    "    response_json = json.loads(response)\n",
    "    if response_json:\n",
    "        for i in range(len(response_json[\"results\"])):\n",
    "            if response_json[\"results\"][i][\"datatype\"] == \"PRCP\":\n",
    "                rain.append(response_json[\"results\"][i][\"value\"])\n",
    "            elif response_json[\"results\"][i][\"datatype\"] == \"TMAX\":\n",
    "                temp_max.append(response_json[\"results\"][i][\"value\"])\n",
    "            elif response_json[\"results\"][i][\"datatype\"] == \"TMIN\":\n",
    "                temp_min.append(response_json[\"results\"][i][\"value\"])\n",
    "    else:\n",
    "        rain.append(None)\n",
    "        temp_max.append(None)\n",
    "        temp_min.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will then add these rows to the resulting dataframe. In the event the json calls are not successfully completed (due to an API call threshold reached or other error) I will pad the resulting arrays with `0`'s to be the correct length and allow successful addition to the dataframe. Finally, the rain and temperature data is added to the `attendance_data` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = rain + [None] * (attendance_data.shape[0] - len(rain))\n",
    "temp_max = temp_max + [None] * (attendance_data.shape[0] - len(temp_max))\n",
    "temp_min = temp_min + [None] * (attendance_data.shape[0] - len(temp_min))\n",
    "\n",
    "attendance_data[\"rain\"] = rain\n",
    "attendance_data[\"max_temp\"] = temp_max\n",
    "attendance_data[\"min_temp\"] = temp_min\n",
    "\n",
    "print(\"The resulting dataset has a shape: \" attendance_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "### Feature engineering\n",
    "\n",
    "Since I want to compare attendance accross multiple programs, I needed to create a metric that could be compared. The dataset includes attedance counts which can not be used across stadiums that fit different numbers of people. Therefore I created the `attendance_percentage` metric to normalize for stadium size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data[\"attendance_percentage\"] = attendance_data[\"attendance\"] / attendance_data[\"capacity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to look at team success in relation to attendance which I will do using rankings data which was also included in the college football dataset available through the rankings endpoint. \n",
    "\n",
    "The rankings data call takes the following parameters:  \n",
    "\n",
    "| name     | required | description |\n",
    "| ---------|----------|-------------|\n",
    "| year     | yes      | year/season filter for games |\n",
    "| week     | no       | week filter |\n",
    "| seasonType| no | season type filter|\n",
    "\n",
    "I wanted all game data so I only passed the year parameter. This returns all regular season games from that year in the following format:  \n",
    "\n",
    "```json\n",
    " {\n",
    "    \"season\": 2019,\n",
    "    \"seasonType\": \"regular\",\n",
    "    \"week\": 8,\n",
    "    \"polls\": [\n",
    "      {\n",
    "        \"poll\": \"AP Top 25\",\n",
    "        \"ranks\": [\n",
    "          {\n",
    "            \"rank\": 1,\n",
    "            \"school\": \"Alabama\",\n",
    "            \"conference\": \"SEC\",\n",
    "            \"firstPlaceVotes\": 30,\n",
    "            \"points\": 1503\n",
    "          },\n",
    "          {\n",
    "            \"rank\": 2,\n",
    "            \"school\": \"LSU\",\n",
    "            \"conference\": \"SEC\",\n",
    "            \"firstPlaceVotes\": 12,\n",
    "            \"points\": 1449\n",
    "          },\n",
    "          .....\n",
    "       }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "I wil then make the API calls to the following endpoint for all of years I am looking at using the `make_and_save_api_call` function defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_endpoint = \"https://api.collegefootballdata.com/rankings?year={year}\"\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2012\"}, \"raw_data/rankings_endpoint_2012.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2013\"}, \"raw_data/rankings_endpoint_2013.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2014\"}, \"raw_data/rankings_endpoint_2014.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2015\"}, \"raw_data/rankings_endpoint_2015.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2016\"}, \"raw_data/rankings_endpoint_2016.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2017\"}, \"raw_data/rankings_endpoint_2017.json\")\n",
    "make_and_save_api_call(rankings_endpoint, {\"year\": \"2018\"}, \"raw_data/rankings_endpoint_2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I will read these into DataFrames in the same manner as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings_2012 = pd.read_json(\"raw_data/rankings_endpoint_2012.json\")\n",
    "rankings_2013 = pd.read_json(\"raw_data/rankings_endpoint_2013.json\")\n",
    "rankings_2014 = pd.read_json(\"raw_data/rankings_endpoint_2014.json\")\n",
    "rankings_2015 = pd.read_json(\"raw_data/rankings_endpoint_2015.json\")\n",
    "rankings_2016 = pd.read_json(\"raw_data/rankings_endpoint_2016.json\")\n",
    "rankings_2017 = pd.read_json(\"raw_data/rankings_endpoint_2017.json\")\n",
    "rankings_2018 = pd.read_json(\"raw_data/rankings_endpoint_2018.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [AP Top 25 Poll](https://apnews.com/APTop25CollegeFootballPoll) is one of the most common and therefore I will use it as my proxy for success. This poll is released every week of the college football season, ranking the top 25 teams. There are roughly 130 college football programs so this is a good proxy of the elite programs at a current point of the season. \n",
    "\n",
    "However, I really want an indicator of if the team is ranked going into a particular game or not, rather than a list of the top 25 teams. Therefore, I will collect only data from this poll and derive a field to indicate if teams were ranked or not for each game.\n",
    "\n",
    "First, I will go through the rankings API results and create lists for the teams included in the AP Top 25 poll each week as well as the corresponding season year and week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [rankings_2012, rankings_2013, rankings_2014, rankings_2015, rankings_2016, rankings_2017, rankings_2018]\n",
    "\n",
    "ap_polls = []\n",
    "years = []\n",
    "weeks = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for index, row in dataset.iterrows():\n",
    "        weeks.append(row.week)\n",
    "        years.append(row.season)\n",
    "        polls = row.polls\n",
    "        for i in range(len(polls)):\n",
    "            # get poll names and only save AP Top 25 results\n",
    "            if polls[i][\"poll\"] == \"AP Top 25\":\n",
    "                ranked_teams = polls[i][\"ranks\"]\n",
    "                schools = []\n",
    "                for ranking in ranked_teams:\n",
    "                    schools.append(ranking[\"school\"])\n",
    "                ap_polls.append(schools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will then add these lists of ranked teams, years and weeks to a DataFrame called `ap_ranking_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_ranking_data = pd.DataFrame()\n",
    "ap_ranking_data[\"ap_poll\"] = ap_polls \n",
    "ap_ranking_data[\"season\"] = years\n",
    "ap_ranking_data[\"week\"] = weeks\n",
    "ap_ranking_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now loop through the `attendance_data` DataFrame and add an indicator for both the home and away team in each game to denote if they were ranked when the game occured or not. I will do so by checking to see if the team was included in the AP Top 25 teams for the season and week that the game occurred. A 1 will indicate that the team was ranked when the game occurred and a 0 will indicate the team was unranked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_team_ranking_indicator = []\n",
    "away_team_ranking_indicator = []\n",
    "\n",
    "for index, row in attendance_data.iterrows():\n",
    "    week_rankings = ap_ranking_data[(ap_ranking_data[\"season\"] == row.season) & (ap_ranking_data[\"week\"] == row.week)]\n",
    "    if row.home_team in week_rankings.ap_poll.iloc[0]:\n",
    "        home_team_ranking_indicator.append(1)\n",
    "    else:\n",
    "        home_team_ranking_indicator.append(0)\n",
    "        \n",
    "    if row.away_team in week_rankings.ap_poll.iloc[0]:\n",
    "        away_team_ranking_indicator.append(1)\n",
    "    else:\n",
    "        away_team_ranking_indicator.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will add these two ranking indicators to the `attendance_data` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data[\"home_team_ranked_ind\"] = home_team_ranking_indicator\n",
    "attendance_data[\"away_team_ranked_ind\"] = away_team_ranking_indicator\n",
    "attendance_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then clean up the data for analysis. First we will filter out any attendance percentages > 1.25, meaning the attendance exceeded the stadium capacity by over 125% which seems highly infeasible and is likely either a data recording error or anomalous event we can exclude for the purposes of this analysis. Then we multiply this feature by 100 so the values are genuine percentages and easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data = attendance_data[attendance_data[\"attendance_percentage\"] <= 1.25]\n",
    "attendance_data[\"attendance_percentage\"] = attendance_data[\"attendance_percentage\"] * 100.0\n",
    "\n",
    "print(\"This leaves us with %s rows in the dataset\" %str(len(attendance_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will convert the temperature values into Fahrenheit for interpretable analysis using the [standard conversion factor](https://www.rapidtables.com/convert/temperature/how-celsius-to-fahrenheit.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data_with_rain_data[\"max_temp\"] = ((attendance_data_with_rain_data[\"max_temp\"] / 10.0) * (9.0/5.0)) + 32\n",
    "attendance_data_with_rain_data[\"min_temp\"] = ((attendance_data_with_rain_data[\"min_temp\"] / 10.0) * (9.0/5.0)) + 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now cleaned and ready for my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "\n",
    "### Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I will write the resulting DataFrame to a csv saved in the `processed_data` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attendance_data.to_csv(\"processed_data/seasons_2012-2018_attendance_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of this dataset is conducted in the Data Analysis notebook in this same repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
